{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow Notebook\n",
    ">This notebook is for building modular models using MLFlow\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os, mlflow, json, subprocess, time\n",
    "from collections import defaultdict\n",
    "from IPython.display import Javascript\n",
    "from sys import modules as m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for the configuration\n",
    "def build_mlp_entrypoints(MLP_entry_points, MLP_entrys = defaultdict(lambda: defaultdict(dict))):\n",
    "    for ep in MLP_entry_points:\n",
    "        MLP_entrys[ep][\"parameters\"][\"data_file\"] = MLP_data_file\n",
    "        MLP_entrys[ep][\"command\"] = \"python \"+MLP_script_name+\" -r {regularization} {data_file}\"\n",
    "    return MLP_entrys\n",
    "\n",
    "def build_MLproject_file(name, env, entry, out=\"MLproject\"):\n",
    "    if \".py\" or \".ipynb\" in name:\n",
    "        name = ''.join(name.split(\".\")[:-1])\n",
    "        \n",
    "    entry = json.loads(json.dumps(entry)) if type(entry) is not dict else entry\n",
    "    _mlp={\"name\":name,\"conda_env\":env,\"entry_points\":entry}\n",
    "    write_ordered_yaml_from_dict(_mlp, out)\n",
    "            \n",
    "def write_ordered_yaml_from_dict(dictionary, out):\n",
    "    with open(out, 'w') as f:\n",
    "        for k,v in dictionary.items(): #this is to keep order in yaml\n",
    "            yaml.dump({k:v},f,default_flow_style=False)\n",
    "            \n",
    "def introspect_dependencies():\n",
    "    return [(mn,getattr(m[mn],'__version__',None)) for mn in list(set(m) & set(globals()))]\n",
    "\n",
    "def generate_conda_file(env_name, channels=[\"defaults\"], out=\"conda.yaml\", deps = None):\n",
    "    deps = [\"{}={}\".format(d[0],d[1]) if d[1] else d[0] for d in introspect_dependencies()]\n",
    "    conda_dict = {\"name\":env_name, \"channels\":channels, \"dependencies\":deps}\n",
    "    if out: write_ordered_yaml_from_dict(conda_dict, out)\n",
    "    return conda_dict\n",
    "\n",
    "def parse_ipynb(file, SEARCH = \"Model\", DELIMITER = \"#\"):\n",
    "    with open(file) as f:\n",
    "        me = json.load(f)\n",
    "    search_level, current_level, start_idx = None, None, None\n",
    "    for idx, cell in enumerate(me['cells']):\n",
    "        if cell['cell_type'] == \"markdown\":\n",
    "            #print(idx)\n",
    "            if len(cell['source']) < 1:\n",
    "                break\n",
    "            curname=str(cell['source'][0])\n",
    "            current_level = 0\n",
    "            for n in str(curname):\n",
    "                if n == DELIMITER: current_level+=1\n",
    "                else: break               \n",
    "            if not search_level:\n",
    "                if SEARCH in str(curname):\n",
    "                    search_level = current_level\n",
    "                    start_idx = idx\n",
    "            elif current_level <= search_level:\n",
    "                break\n",
    "    me['cells'] = me['cells'][start_idx:idx]\n",
    "    return (me)\n",
    "\n",
    "        \n",
    "def convert_nb_to(out_type,MLP_nb_name,output_name,build_dir=None,\n",
    "                  section=None,delimiter=\"#\"):\n",
    "    _temp = None\n",
    "    if section:\n",
    "        _temp = \"{}__temp__{}\".format(build_dir, MLP_nb_name)\n",
    "        print(\"Writing temporary file {}\".format(_temp))\n",
    "        with open(_temp,\"w\") as f:\n",
    "            mynb = json.dumps(parse_ipynb(MLP_nb_name, section, delimiter))\n",
    "            f.write(mynb)\n",
    "        MLP_nb_name=_temp\n",
    "    new_name = ''.join(MLP_nb_name.split(\".\")[:-1])+\".\"+''.join(output_name.split(\".\")[-1])\n",
    "    subprocess.call(\"jupyter nbconvert --to {} {}\".format(out_type, MLP_nb_name), shell=True)\n",
    "    print(\"Wrote {} from {}\".format(new_name, MLP_nb_name))\n",
    "    if _temp:\n",
    "        subprocess.call(\"rm {}\".format(_temp), shell=True)\n",
    "        print(\"Cleaning up temporary file {}\".format(_temp))\n",
    "\n",
    "    subprocess.call(\"mv {} {}\".format(new_name, build_dir+output_name), shell=True)\n",
    "    print(\"Renamed {} to {}\".format(new_name, build_dir+output_name))\n",
    "        \n",
    "def build_package(MLP_name,MLP_nb_name, MLP_script_name, MLP_build_dir=None,MLP_conda_channels=[\"defaults\"],\n",
    "                  MLP_model_section=None, MLP_readme_section=None,MLP_env=\"conda.yaml\",  MLP_conda_file=\"conda.yaml\",\n",
    "                 MLP_mlproject_file=\"MLproject\", MLP_entry_points = MLP_entry_points):\n",
    "    \n",
    "    build_dir = create_build_directory(MLP_build_dir, timestamp=True)\n",
    "    convert_nb_to(\"markdown\",MLP_nb_name,\"README.md\",build_dir=build_dir);\n",
    "    convert_nb_to(\"script\",MLP_nb_name,MLP_script_name,build_dir=build_dir, section=MLP_model_section)\n",
    "    build_MLproject_file(MLP_name, MLP_env, MLP_entry_points, out=build_dir+MLP_mlproject_file)\n",
    "    generate_conda_file(MLP_env, channels=MLP_conda_channels, out=build_dir+MLP_conda_file)\n",
    "    return build_dir     \n",
    "\n",
    "def create_build_directory(build_dir = None, timestamp=False):\n",
    "    if not build_dir:\n",
    "        return None\n",
    "    if timestamp:build_dir+=time.strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
    "    if not os.path.exists(build_dir):\n",
    "        os.makedirs(build_dir)\n",
    "    return build_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Configuration\n",
    "##### Parameters\n",
    "- __MLP_name__ - name of your project\n",
    "- __MLP_data_file__ - Location of data (Full path, unless copying to build)\n",
    "- __MLP_entry_points__ - Dict of entry points. Detailed below.\n",
    "- __MLP_nb_name__ - Notebook name. Defaults to current nb.\n",
    "- __MLP_script_name__ - Name of the script to build to.\n",
    "\n",
    "\n",
    "##### Defaults\n",
    "\n",
    "- __MLP_model_section__ - Directory to build each test in.\n",
    "- __MLP_readme_section__ - Directory to build each test in.\n",
    "- __MLP_env__ - conda environment. This defaults to your \"conda.yaml\"\n",
    "- __MLP_mlproject_file__ - name of script (default is MLP_nb_name)-(.ipynb)+(.py)\n",
    "- __MLP_conda_channels__ - name of script (default is MLP_nb_name)-(.ipynb)+(.py)\n",
    "- __MLP_conda_file__ - name of script (default is MLP_nb_name)-(.ipynb)+(.py)\n",
    "\n",
    "##### Entry Points\n",
    "- __MLP_entry_points__ - build_mlp_entrypoints() function converts a list into a defaultdict with the standard settings and can be added to in the following fashion:\n",
    "    - ```MLP_entry_points[\"main\"][\"parameters\"][\"regularization\"] = \"{type: float, default: 0.1}\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_name = \"MLFlow_Test\"\n",
    "MLP_data_file = \"data.csv\"\n",
    "MLP_build_dir = \"build/\" + MLP_name + \"/\"\n",
    "MLP_nb_name = \"mlflow_notebook.ipynb\"\n",
    "MLP_script_name = ''.join(MLP_nb_name.split(\".\")[:-1]) + \".py\"\n",
    "MLP_entry_points = build_mlp_entrypoints([\"main\", \"validate\"])\n",
    "\n",
    "# Defaults\n",
    "MLP_model_section = \"Model\"\n",
    "MLP_readme_section = None\n",
    "MLP_env = \"conda.yaml\"\n",
    "MLP_mlproject_file=\"MLproject\"\n",
    "MLP_conda_channels = [\"defaults\"]\n",
    "MLP_conda_file = \"conda.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MLflow Package\n",
    "To build a package, you simply call the build_package function with your settings. It will generate all files required for MLFlow into a new directory. If your build_dir is set to \"\", it will generate the files locally.\n",
    "\n",
    "\n",
    "```build_package(MLP_name,MLP_nb_name, MLP_script_name, MLP_build_dir, MLP_model_section=MLP_model_section)```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Notebook to  Script/Markdown Conversion\n",
    "Converts this notebook to a .py file\n",
    "```\n",
    "convert_nb_to_script(notebook_name = \"notebook.ipynb\", script_name = \"test.py\", section=None, include_all_deps=True)\n",
    "```\n",
    "\n",
    "Specifying ```section``` will search markdown for a certain section (as delimited by ##) and only write that to a notebook. ```include_all_deps``` also adds any import statements included anywhere else in the notebook.\n",
    "\n",
    "##### MLproject File Generation\n",
    "Writes the MLproject file with user parameters (set above, in \"Project Config\")\n",
    "\n",
    "```\n",
    "build_MLproject_file(name, env, entry, out=\"MLproject\")\n",
    "```\n",
    "\n",
    "##### Conda File Generation \n",
    "Writes dependencies into a dependencies file.\n",
    "```deps``` takes a list of module names (name or name=4.2), or uses introspection to determine imported modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote mlflow_notebook.md from mlflow_notebook.ipynb\n",
      "Renamed mlflow_notebook.md to build/test/20180712-023944/README.md\n",
      "Writing temporary file build/test/20180712-023944/__temp__mlflow_notebook.ipynb\n",
      "Wrote build/test/20180712-023944/__temp__mlflow_notebook.py from build/test/20180712-023944/__temp__mlflow_notebook.ipynb\n",
      "Cleaning up temporary file build/test/20180712-023944/__temp__mlflow_notebook.ipynb\n",
      "Renamed build/test/20180712-023944/__temp__mlflow_notebook.py to build/test/20180712-023944/mlflow_notebook.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'build/test/20180712-023944/'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_package(MLP_name,MLP_nb_name, MLP_script_name, MLP_build_dir, MLP_model_section=MLP_model_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "\n",
    "Per the standard configuration (MLP_model_section), anything under the heading Model will be built into the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-e0168f112ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mwine_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wine-quality.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwine_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(40)\n",
    "\n",
    "    # Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n",
    "    wine_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"wine-quality.csv\")\n",
    "    data = pd.read_csv(wine_path)\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train, test = train_test_split(data)\n",
    "\n",
    "    # The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "    train_x = train.drop([\"quality\"], axis=1)\n",
    "    test_x = test.drop([\"quality\"], axis=1)\n",
    "    train_y = train[[\"quality\"]]\n",
    "    test_y = test[[\"quality\"]]\n",
    "\n",
    "    alpha = float(sys.argv[1]) if len(sys.argv) > 1 else 0.5\n",
    "    l1_ratio = float(sys.argv[2]) if len(sys.argv) > 2 else 0.5\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "        lr.fit(train_x, train_y)\n",
    "\n",
    "        predicted_qualities = lr.predict(test_x)\n",
    "\n",
    "        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "        print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
    "        print(\"  RMSE: %s\" % rmse)\n",
    "        print(\"  MAE: %s\" % mae)\n",
    "        print(\"  R2: %s\" % r2)\n",
    "\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "mlflow.sklearn.log_model(lr, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
